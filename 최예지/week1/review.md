---
name: ğŸ“ ë¦¬ë·° ê³¼ì œ
about: 2ì¥ ML ê¸°ë³¸ì§€ì‹
title: '[Week 1] ì£¼ì°¨ ë¦¬ë·° - ìµœì˜ˆì§€'
labels: ['review']
assignees: ''
---

## ì£¼ì œ
<!-- ì´ë²ˆ ì£¼ì°¨ì— ë‹¤ë£¬ ì£¼ìš” ì£¼ì œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš” -->
- ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì •í™•í•œ ê°œë…ê³¼ ì°¨ì´
- Q) ë”¥ëŸ¬ë‹ì´ í•­ìƒ ë¨¸ì‹ ëŸ¬ë‹ë³´ë‹¤ ì¢‹ì€ê°€?
- ê²½ì‚¬í•˜ê°•ë°¥ì˜ ë‹¤ì–‘í•œ ìµœì í™” ë°©ë²•ë“¤ê³¼ ì´ë“¤ì˜ ì°¨ì´ì 
- Q) adamì´ ë§ì´ ì“°ì´ëŠ” ì´ìœ ëŠ”?

## ë‚´ìš©
<!-- ì£¼ìš” ê°œë…ê³¼ ë‚´ìš©ì„ ì •ë¦¬í•´ì£¼ì„¸ìš” -->

### 1. ë¨¸ì‹ ëŸ¬ë‹ vs ë”¥ëŸ¬ë‹
- **ë¨¸ì‹ ëŸ¬ë‹**ì˜ ì •ì˜: ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•œ ë‹¤ìŒ, í•™ìŠµí•œ ê²ƒì„ ì ìš©í•´ ì •ë³´ì— ì…ê°í•œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ í¬í•¨í•˜ëŠ” ì¸ê³µ ì§€ëŠ¥ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜

- **ë”¥ëŸ¬ë‹**ì˜ ì •ì˜: ì•Œê³ ë¦¬ì¦˜ì„ ê³„ì¸µìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ìì²´ì ìœ¼ë¡œ ë°°ìš°ê³  ë˜‘ë˜‘í•œ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ” 'ì¸ê³µ ì‹ ê²½ë§'ì„ ë§Œë“œëŠ” ë¨¸ì‹  ëŸ¬ë‹ì˜ í•˜ìœ„ë¶„ì•¼

*ì¶œì²˜: https://www.zendesk.kr/blog/machine-learning-and-deep-learning/


### 2. ë”¥ëŸ¬ë‹

a. **í¼ì…‰íŠ¸ë¡  (Perceptron)**: ì´ˆê¸°ì˜ ì¸ê³µ ì‹ ê²½ë§

ì‹¤ì œ ë‡Œë¥¼ êµ¬ì„±í•˜ëŠ” ì‹ ê²½ ì„¸í¬ ë‰´ëŸ°ì˜ ë™ì‘ê³¼ ìœ ì‚¬í•˜ì—¬, ë‹¤ìˆ˜ì˜ ì…ë ¥ë¶€í„° í•˜ë‚˜ì˜ ê²°ê³¼ë¥¼ ë‚´ë³´ë‚´ëŠ” ì•Œê³ ë¦¬ì¦˜. 

ê°ê°ì˜ ì…ë ¥ê°’ì—ëŠ” ê°ê°ì˜ ê°€ì¤‘ì¹˜ê°€ ì¡´ì¬í•˜ëŠ”ë°, ì´ë•Œ ê°€ì¤‘ì¹˜ì˜ ê°’ì´ í¬ë©´ í´ìˆ˜ë¡ í•´ë‹¹ ì…ë ¥ ê°’ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ê° ì…ë ¥ê°’ì´ ê°€ì¤‘ì¹˜ì™€ ê³±í•´ì ¸ì„œ ì¸ê³µ ë‰´ëŸ°ì— ë³´ë‚´ì§€ê³ , ê° ì…ë ¥ê°’ê³¼ ê·¸ì— í•´ë‹¹ë˜ëŠ” ê°€ì¤‘ì¹˜ì˜ ê³±ì˜ ì „ì²´ í•©ì´ ì„ê³„ì¹˜(threshold)ë¥¼ ë„˜ìœ¼ë©´ ì¢…ì°©ì§€ì— ìˆëŠ” ì¸ê³µ ë‰´ëŸ°ì€ ì¶œë ¥ ì‹ í˜¸ë¡œì„œ 1ì„ ì¶œë ¥í•˜ê³ , ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” 0ì„ ì¶œë ¥í•œë‹¤ (Step function).

- ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ : ì…ë ¥ì¸µ, ì¶œë ¥ì¸µ ë‘ ë‹¨ê³„ë¡œë§Œ ì´ë£¨ì–´ì§
- ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ : ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë“±ì¥. ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§ 

b. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)

![ì‹ ê²½ë§ êµ¬ì¡°](https://github.com/user-attachments/assets/8df69e7c-981b-4609-ab03-3955abc796c5)

- DNN(Deep Neural Network, ì‹¬ì¸µ ì‹ ê²½): ì€ë‹‰ì¸µì´ 2ê°œ ì´ìƒì¸ ì‹ ê²½ë§


c. ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•
- ê° ë…¸ë“œ ë˜ëŠ” ì€ë‹‰ì¸µì—ëŠ” ì¶œë ¥ê³¼ì˜ ê´€ê³„ ê°•ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ë˜ë©°, ëª¨ë¸ì´ í•™ìŠµí•˜ë©´ì„œ ì´ ê°€ì¤‘ì¹˜ëŠ” ì§€ì†ì ìœ¼ë¡œ ì¡°ì •ëœë‹¤.
- ë”¥ëŸ¬ë‹ ê¸°ìˆ ì€ ì€ë‹‰ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ ë²”ì£¼ë¥¼ ì ì§„ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
ë¨¼ì € ë¬¸ìì™€ ê°™ì€ ë‚®ì€ ìˆ˜ì¤€ì˜ ë²”ì£¼ë¥¼ í•™ìŠµí•œ í›„, ë‹¨ì–´ì™€ ë¬¸ì¥ì²˜ëŸ¼ ì ì°¨ ë†’ì€ ìˆ˜ì¤€ì˜ ë²”ì£¼ë¥¼ ì •ì˜í•œë‹¤.
ì´ëŸ¬í•œ 'ë°ì´í„°ì—ì„œ ê³ ìˆ˜ì¤€ íŠ¹ì§•ì„ ì ì§„ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹'ì€ ë³„ë„ì˜ ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ì´ë‚˜ ë³µì¡í•œ íŠ¹ì§• ì¶”ì¶œ ê³¼ì • ì—†ì´ë„ íš¨ê³¼ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ë©°, ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ê³¼ì˜ í° ì°¨ì´ì ì´ë¼ í•  ìˆ˜ ìˆë‹¤.
- ë”¥ëŸ¬ë‹ì˜ í° ì¥ì ì€ ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤.
ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìŠ¤í¬ì¸ ì¹´ì— ë¹„ìœ í•˜ë“¯, ì¼ì • ê±°ë¦¬ ì´ìƒ ê°€ì†í•´ì•¼ ìµœê³  ì†ë„ì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤.
ë§ˆì°¬ê°€ì§€ë¡œ, ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ ë°œíœ˜í•˜ì§€ ëª»í•˜ë©°, ë°ì´í„°ê°€ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œëŠ” ì˜¤íˆë ¤ ë‹¨ìˆœí•œ ëª¨ë¸ì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤.
ì´ëŠ” ìš¸ì°½í•œ ìˆ²ê¸¸ì—ì„œëŠ” í˜ë¼ë¦¬(ì˜ˆ: í•©ì„±ê³± ì‹ ê²½ë§)ë³´ë‹¤ ì‚°ì•…ìì „ê±°(ì˜ˆ: CART ê²°ì • íŠ¸ë¦¬)ê°€ ë” ì í•©í•œ ê²ƒê³¼ ê°™ì€ ì›ë¦¬ë‹¤.
- ë”¥ëŸ¬ë‹ì€ ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ ë‹¬ë¦¬ ê³ ì„±ëŠ¥ ë¨¸ì‹ ì´ í•„ìš”í•˜ë©°, í˜„ì¬ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í–‰í•˜ëŠ” ë° GPUëŠ” í•„ìˆ˜ì ì¸ ìš”ì†Œì´ë‹¤.
  
![Why_DL](https://github.com/user-attachments/assets/a8bd6a81-247c-45db-8cde-6a5a1d5395a8)

*ì¶œì²˜: https://medium.com/towards-data-science/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063


### 3. ë”¥ëŸ¬ë‹ì´ ë¬´ì¡°ê±´ ë¨¸ì‹ ëŸ¬ë‹ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë‚´ëŠ”ê°€?

ê·¸ë ‡ì§€ ì•Šë‹¤. ì£¼ë¡œ ë”¥ëŸ¬ë‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì— ì„ í˜¸ëœë‹¤:
1. ë°ì´í„°ê°€ ë°©ëŒ€í•  ê²½ìš° ë”¥ëŸ¬ë‹ì´ ë‹¤ë¥¸ ê¸°ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ë§Œ, ë°ì´í„°ê°€ ì ì„ ë•ŒëŠ” ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ë” ì í•©í•˜ë‹¤.
2. ë”¥ëŸ¬ë‹ ê¸°ë²•ì€ í•©ë¦¬ì ì¸ í•™ìŠµ ì‹œê°„ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ê³ ì„±ëŠ¥ ì¸í”„ë¼ê°€ ê°–ì¶°ì ¸ì•¼ í•œë‹¤.
3. íŠ¹ì§• ì¶”ì¶œì— ëŒ€í•œ ë„ë©”ì¸ ì§€ì‹ì´ ë¶€ì¡±í•  ê²½ìš°, ë”¥ëŸ¬ë‹ì€ ë³„ë„ì˜ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ì— ëŒ€í•œ ê³ ë¯¼ ì—†ì´ë„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¨¸ì‹ ëŸ¬ë‹ë³´ë‹¤ ë” ì í•©í•˜ë‹¤ë‹¤.

### 4. ê²½ì‚¬í•˜ê°•ë²•(gradient descent method)ì´ë€?
![GD ìˆ˜ì‹](Gradient_Descent_Algorithm_ìˆ˜ì‹.png)

ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì€ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ëª¨ë¸ì´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ë©° í•™ìŠµí•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤. 
ì´ë“¤ ì¤‘ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²½ì‚¬í•˜ê°•ë²•ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ í™œìš©í•˜ì—¬ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ë©° 1ì°¨ ê·¼ì‚¿ê°’ì„ ì°¾ëŠ”ë‹¤.
- ê¸°ë³¸ ê°œë…: í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(ê²½ì‚¬)ë¥¼ êµ¬í•˜ê³  ê²½ì‚¬ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê³„ì† ì´ë™ì‹œì¼œ ê·¹ê°’(ê¸°ìš¸ê¸°=0)ì— ì´ë¥¼ ë•Œê¹Œì§€ ë°˜ë³µì‹œí‚´
- ê²½ì‚¬í•˜ê°•ë²•ì€ í•˜ë‚˜ì˜ ë°˜ë³µ ì•ˆì—ì„œ ë‹¤ë£° ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸°ì— ë”°ë¼ ì„¸ ì¢…ë¥˜ë¡œ ë‚˜ë‰œë‹¤.
  
![GD 1](Gradient_descent.png)


### 5. ê²½ì‚¬í•˜ê°•ë²•ì˜ ì„¸ ì¢…ë¥˜ (BGD, SGD, MGD)
a. **Batch Gradient Descent Method (ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•)**

![BGD ìˆ˜ì‹](BGD_ìˆ˜ì‹.png)

- *batch: ê°€ì¤‘ì¹˜ ë“±ì˜ ë§¤ê°œ ë³€ìˆ˜ì˜ ê°’ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ì–‘ -> ì „ì²´ ë°ì´í„° or ì •í•´ì¤€ ì–‘ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ë§¤ê°œë³€ìˆ˜ì˜ ê°’ ì¡°ì • ê°€ëŠ¥
- ì¼ë°˜ì ì¸ ê²½ì‚¬í•˜ê°•ë²•
- ê° ë°˜ë³µë§ˆë‹¤ ëª¨ë“  í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•
- 1 iteration ì•ˆì—ì„œ ì „ì²´ ë°ì´í„° ìƒ˜í”Œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°
- ê¸°ìš¸ê¸° ê³„ì‚° í›„ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•œ í›„ ë‹¤ì‹œ ìµœì ì˜ í•´ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê°™ì€ ê³„ì‚° ë°˜ë³µ
- ìˆ˜ë ´í•˜ëŠ” ë° ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦°ë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.

b. **Stochastic Gradient Descent (í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)**

![SGD ìˆ˜ì‹](SGD_ìˆ˜ì‹.png)

- ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì¡°ì • ì‹œ ëœë¤ìœ¼ë¡œ ì„ íƒí•œ í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ëŠ” ë°©ë²•
- ê°€ì¥ ë¨¼ì € ì „ì²´ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¬´ì‘ìœ„í™”
- ê·¸ ë‹¤ìŒ, í•˜ë‚˜ì˜ iteration ì•ˆì—ì„œ í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´ì„œ ì—…ë°ì´íŠ¸ ì§„í–‰
- ì²« ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ -> ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ ... -> më²ˆì§¸ ë°ì´í„° ê¸°ìš¸ê¸° ê³„ì‚° í›„ ì—…ë°ì´íŠ¸
- ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì— ë„ë‹¬í•  ë•Œê¹Œì§€ í•´ë‹¹ ê³„ì‚°ì„ ë°˜ë³µ
- ì ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ BGDë³´ë‹¤ ê³„ì‚°ì´ ë¹ ë¥´ë‹¤.
- ìµœì†Ÿê°’ì¸ (0,0)ê¹Œì§€ ì§€ê·¸ì¬ê·¸ë¡œ ì´ë™í•˜ë¯€ë¡œ ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. (parameter ë³€ê²½ í­ì´ ë¶ˆì•ˆì •í•˜ë‹¤)
- SGDì˜ ë‹¨ì ì„ ê°œì„ í•´ì£¼ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” Momentum, Adagrad, Adamì˜ ì„¸ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.
  
[SGD.png]

c. **Mini Batch Gradient Descent (ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•)**

![MBGD_ìˆ˜ì‹](MBGD_ìˆ˜ì‹.png)

- ì „ì²´ ë°ì´í„°ì…‹ì„ mini-batch ì—¬ëŸ¬ ê°œë¡œ ë‚˜ëˆˆ ë’¤, ë¯¸ë‹ˆ ë°°ì¹˜ í•œ ê°œë§ˆë‹¤ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. ì´ë•Œ í‰ê·  ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•
- ì „ì²´ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ ë¹ ë¥´ë‹¤.
- ë³€ê²½ í­ì´ SGDì— ë¹„í•´ ì•ˆì •ì ì´ê³  ì†ë„ë„ ë¹ ë¥´ë‹¤.

d. **SGD with momentum**

![momentum ìˆ˜ì‹](momentum_SGD_ìˆ˜ì‹.png)

- ê²½ì‚¬ í•˜ê°•ë²•ì— ê´€ì„±ì„ ë”í•´ì¤Œ
- **momentum**: SGDì—ì„œ ê³„ì‚°ëœ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ì— 1 step ì „ì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ê°’ì„ ì¼ì •í•œ ë¹„ìœ¨ë§Œí¼ ë°˜ì˜
- ì´ëŸ¬í•œ momentumì˜ ë°©ì‹ì„ ì ìš©í•˜ë©´, ë§ˆì¹˜ ì–¸ë•ì—ì„œ ê³µì´ ë‚´ë ¤ì˜¬ ë•Œ, ì¤‘ê°„ì— ì‘ì€ ì›…ë©ì´ì— ë¹ ì§€ë”ë¼ë„ ê´€ì„±ì˜ í˜ìœ¼ë¡œ ë„˜ì–´ì„œëŠ” íš¨ê³¼ë¥¼ ì¤„ ìˆ˜ ìˆë‹¤.
- local minimumì— ë„ë‹¬í–ˆì„ ë•Œ, ê´€ì„±ì˜ í˜ì„ ë¹Œë ¤ ê°’ì„ ì¡°ì ˆí•˜ì—¬ ë¡œì»¬ ë¯¸ë‹ˆë©ˆì—ì„œ íƒˆì¶œ -> ê¸°ì¡´ì˜ ê²½ì‚¬ í•˜ê°•ë²•ì´ë¼ë©´ ë¡œì»¬ ë¯¸ë‹ˆë©ˆì„ ê¸€ë¡œë²Œ ë¯¸ë‹ˆë©ˆìœ¼ë¡œ ì˜ëª» ì¸ì‹í–ˆì„ ê²ƒ! 

![momentum 1](momentumSGD_1.png)

![momentum 2](momentumSGD_2.png)

#### ì •ë¦¬

![ì •ë¦¬](GD_SGD_MBGD.png)


### 6. ë‹¤ì–‘í•œ ìµœì í™” ë°©ë²•ë“¤
a. **AdaGrad**

![ì•„ë‹¤ê·¸ë¼ë“œ ìˆ˜ì‹](Adagrad_ìˆ˜ì‹.png)

- *í•™ìŠµë¥  ê°ì†Œ (learning rate decay): í•™ìŠµë¥ ì„ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ ; í•™ìŠµ ì§„í–‰ ì¤‘ì— learning rateì„ ì¤„ì—¬ê°€ëŠ” ë°©ë²• (ì²˜ìŒì— í¬ê²Œ í•™ìŠµ -> ì ì°¨ ì‘ê²Œ í•™ìŠµ)
- ê¸°ìš¸ê¸°ì˜ í¬ê¸°ì— ë”°ë¼ learning rateë¥¼ ìë™ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜.
  ë§¤ í•™ìŠµ ë‹¨ê³„ì—ì„œ ê° ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜)ì— ëŒ€í•œ ê¸°ìš¸ê¸°ì˜ ì œê³±ì„ ëˆ„ì í•´ì„œ ê¸°ë¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤.
- ë³€í™”ê°€ ì ì€ ë°©í–¥ (ê¸°ìš¸ê¸°ê°€ ì‘ì€ ë°©í–¥)ì—ì„œëŠ” í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” íš¨ê³¼
- ë³€í™”ê°€ ê¸‰ê²©í•œ ë°©í–¥ (ê¸°ìš¸ê¸°ê°€ í° ë°©í–¥)ì—ì„œëŠ” í•™ìŠµ ì†ë„ë¥¼ ì¤„ì´ëŠ” íš¨ê³¼
- íŠ¹ì • ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ, í˜„ì¬ ê¸°ìš¸ê¸°ë¥¼ ì´ì „ê¹Œì§€ì˜ ê¸°ìš¸ê¸° ì œê³±í•©ì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆˆë‹¤
- ìˆ˜ì‹ì—ì„œ ë³´ì´ëŠ”ëŒ€ë¡œ, gì˜ ë£¨íŠ¸ ë¡œ ë‚˜ëˆ„ì–´ì§„ í˜•íƒœë¡œ ì—…ë°ì´íŠ¸ ë˜ë¯€ë¡œ, ì‘ì€ ê¸°ìš¸ê¸°ë¥¼ ê°€ì¥ ë°©í–¥ì€ ìƒëŒ€ì ìœ¼ë¡œ í° ê°’ìœ¼ë¡œ ì¡°ì •ë˜ê³ , í° ê¸°ìš¸ê¸°ë¥¼ ê°€ì§„ ë°©í–¥ì€ ì‘ì€ ê°’ìœ¼ë¡œ ì¡°ì •ëœë‹¤.
- ë”°ë¼ì„œ í•™ìŠµì„ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ê¸°ìš¸ê¸° ì œê³±ì˜ ëˆ„ì  í•©ì´ ì»¤ì§€ë©´, í•™ìŠµë¥ ì´ ì ì°¨ ì¤„ì–´ë“¤ì–´ 0ì— ìˆ˜ë µí•´ í•™ìŠµì´ ë©ˆì¶œ ìˆ˜ë„ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. -> ì´ë¥¼ ë³´ì™„í•œê²Œ RMSProp

b. **RMSProp**

![RMSProp ìˆ˜ì‹](RMSprop_ìˆ˜ì‹.png)

- *ì§€ìˆ˜ì´ë™í‰ê· : ê³¼ê±°ì˜ ê¸°ìš¸ê¸°ë“¤ì„ ë˜‘ê°™ì´ ë”í•´ê°€ëŠ” ê²ƒì´ ì•„ë‹Œ, ë¨¼ ê³¼ê±°ì˜ ê¸°ìš¸ê¸°ëŠ” ì¡°ê¸ˆ ë°˜ì˜í•˜ê³  ìµœì‹ ì˜ ê¸°ìš¸ê¸°ë¥¼ ë§ì´ ë°˜ì˜
- ê¸°ìš¸ê¸° ì œê³±ì˜ ëˆ„ì ê°’ì´ ê³„ì† ì¦ê°€í•˜ì§€ ì•Šë„ë¡ ì¼ì • ë¹„ìœ¨ë¡œ ê°ì†Œì‹œí‚¤ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” (AdaGradì˜ ë‹¨ì ì„ ê°œì„ í•œ) ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- ê¸°ìš¸ê¸°ê°€ ì‘ì€ ë°©í–¥ì—ì„œëŠ” í•™ìŠµë¥ ì´ ìƒëŒ€ì ìœ¼ë¡œ ì»¤ì ¸ ì—…ë°ì´íŠ¸ê°€ í™œë°œí•˜ê²Œ ì§„í–‰ë¨
- ê¸°ìš¸ê¸°ê°€ í° ë°©í–¥ì—ì„œëŠ” í•™ìŠµë¥ ì´ ì¤„ì–´ë“¤ì–´ ì•ˆì •ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•¨
- AdaGradì˜ ì¥ì ì€ ê°€ì ¸ê°€ë©´ì„œ ë‹¨ì ì„ ë³´ì™„!

b. **Adam (ì•„ë‹´)**

![Adam ìˆ˜ì‹](Adam_ìˆ˜ì‹.png)

- SGD momentum (ê¸°ìš¸ê¸°ì˜ ì´ë™í‰ê· ;momentum ì‚¬ìš©í•˜ì—¬ ì†ë„ ì¡°ì ˆ) ê³¼ RMSProp (ê¸°ìš¸ê¸° ì œê³±ì˜ ëˆ„ì í•©ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë¥ ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •) ì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- 1) ì²« ë²ˆì§¸ momentum ê³„ì‚°
     - ê³¼ê±° ê¸°ìš¸ê¸° ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ìƒˆë¡œìš´ ê¸°ìš¸ê¸° í‰ê·  mì„ ê³„ì‚°
     - ë² íƒ€1 (ë³´í†µ 0.9)ëŠ” ê³¼ê±° ê¸°ìš¸ê¸°ì˜ ì˜í–¥ì„ ì¡°ì ˆí•˜ëŠ” hyperparameter
     - SGD momentum ì²˜ëŸ¼ ê¸°ìš¸ê¸° ë°©í–¥ì„ ë§¤ë„ëŸ½ê²Œ ë§Œë“¦
  2) ë‘ ë²ˆì§¸ momentum ê³„ì‚°
     - ê³¼ê±° ê¸°ìš¸ê¸° ì œê³±ì˜ í‰ê· ì„ ê³„ì‚°í•´ì„œ ê¸°ìš¸ê¸° í¬ê¸°ì— ë”°ë¼ í•™ìŠµë¥ ì„ ì¡°ì ˆ
     - ë² íƒ€2 (ë³´í†µ 0.999)ëŠ” ê¸°ìš¸ê¸° í¬ê¸°ì˜ ë³€í™”ëŸ‰ì„ ì¡°ì ˆí•˜ëŠ” hyperparameter
     - RMSProp ì²˜ëŸ¼ í•™ìŠµë¥ ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆ -> í° ê¸°ìš¸ê¸°ëŠ” ì‘ê²Œ, ì‘ì€ ê¸°ìš¸ê¸°ëŠ” í¬ê²Œ
  3) ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
     - ìœ„ì—ì„œ êµ¬í•œ ë‘ ëª¨ë©˜í…€ì„ ì´ìš©í•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸
- ì²˜ìŒ í•™ìŠµì„ ì‹œì‘í•  ë•Œ m, vê°€ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ ì²« ë²ˆì§¸ ì—…ë°ì´íŠ¸ì—ì„œ ê³„ì‚°ëœ ê°’ì´ ë§¤ìš° ì‘ì„ ìˆ˜ ìˆìŒ -> ì´ˆë°˜ì—ëŠ” ê¸°ìš¸ê¸° ì—…ë°ì´íŠ¸ê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ìœ„í—˜ ì¡´ì¬
  - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ bias correction ì ìš©
  - ì´ëŠ” ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•œë‹¤

### 7. ì™œ Adam ì´ ê°€ì¥ ë§ì´ ì“°ì´ëŠ”ê°€?

ë³´í¸ì ìœ¼ë¡œ ê°„ë‹¨í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ì‘ì€ ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•ŒëŠ”, ì¼ë°˜ì ì¸ Gradient descent ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì í•©í•˜ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.
ê·¸ëŸ¬ë‚˜, Neural Network Architectureì„ ë‹¤ë£° ëŒ€ëŠ” Adamì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ğ›½1 = 0.9, ğ›½2 = 0.999, learning rate = 0.001-0.0001)ë¥¼ ì„¤ì •í–ˆì„ ë•Œ,
ë‹¤ì–‘í•œ ê²½ìš°ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì—ˆë‹¤ëŠ” ì‚¬ë¡€ê°€ ë§ì•„ì§€ë©´ì„œ Adamì„ ë””í´íŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ í”í•´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

### ì°¸ê³  ìë£Œ
- https://medium.com/@ilyurek/optimizers-a-deep-dive-into-gradient-descent-adam-and-beyond-e6a1d00bc9b0
- https://velog.io/@cha-suyeon/DL-%EC%B5%9C%EC%A0%81%ED%99%94-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-RMSProp-Adam
- https://velog.io/@cha-suyeon/DL-%EC%B5%9C%EC%A0%81%ED%99%94-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98
- https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=lego7407&logNo=221681014509
- https://bruders.tistory.com/91
- https://data-science-hi.tistory.com/165
- https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D
- https://www.geeksforgeeks.org/optimization-techniques-for-gradient-descent/
- https://wikidocs.net/60680
- https://artemoppermann.com/optimization-in-deep-learning-adagrad-rmsprop-adam/




