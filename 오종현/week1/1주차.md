# Why Adam optimizer is best? (25.01.22)
> Adam Optimizer가 처음 등장하고 10년이 넘은 지금도 여전히 Adam은 딥러닝 분야에서 가장 많이 이용되는 최적화 알고리즘입니다. 시간이 많이 지났음에도 여러 최적화 알고리즘 중에서 Adam이 여전히 큰 인기를 누리고 있는 이유는 무엇일까요?
---

![enter link description here](https://mblogthumb-phinf.pstatic.net/MjAxOTEwMTdfOCAg/MDAxNTcxMzIzODQwODc2.sGUa3hqpGl_cy3cAFKfFP3aPGd3GLWMkHhSD123dgcYg.kKbblC-TTnXl40YL9dTVEDjzQVnPPZtX_qJHPM_t7U4g.PNG.lego7407/e_6.1.png?type=w800)

기존의 SGD(확률적 경사 하강법)는 여러 가지 문제를 갖고 있었습니다. **학습률이 너무 작으면** 수렴 속도가 느려지고 최적해가 아닌 Local Minimum에 빠질 확률이 높은 반면, **너무 큰 경우**에는 매개변수가 손실 함수의 최솟값에 수렴하지 못하고 진동하는 일조차 발생할 수 있었습니다. 

---
이러한 문제를 해결하기 위해 **Momentum** 이라는 기법이 등장했습니다. 물체가 속도와 방향을 가지듯이, 매개변수의 업데이트에도 이전 변화량(속도)을 반영하여 기존의 고정된 학습률에 따른 한계를 해결하겠다는 아이디어입니다.  

![enter link description here](https://blog.kakaocdn.net/dn/14FtY/btsgnhyd4Oc/bkikJEqSdJVi3BcIh0ADk0/img.png)
공이 굴러 떨어지고 있는 상황을 가정해 봅시다. 공이 경사가 높은 곳에서 낮은 곳으로 이동함에 따라 점점 큰 속도를 가지게 되는 것처럼, 같은 방향으로 이동할 때는 이전의 이동량이 반영이 돼서 점점 더 큰 값이 업데이트가 되게 됩니다. 이로 인해 **Local Minimum에 빠진다 하더라도 이전에 이동했던 모멘텀이 반영이 돼서 이를 빠져나갈 수도 있게 됩니다.**

---
**RMSProp**는 적응형 학습률을 사용하는 최적화 알고리즘입니다. 적절한 학습률을 설정하는 것은 최적화 알고리즘에서 중요한 작업인 만큼, 그 학습률을 학습 도중에 동적으로 변화시키겠다는 아이디어입니다. 

![enter link description here](https://wiki.cloudfactory.com/media/pages/docs/mp-wiki/solvers-optimizers/rmsprop/3d95bb0387-1684131968/contour.webp)

이때 SGD와 같이 sample 하나의 노이즈에 크게 반응하지 않도록 과거의 gradient 값들을 반영하되, 최근 gradient 값에 더 큰 영향을 받는 **지수이동평균**을 이용하여 기울기가 큰 곳에는 학습률을 낮추고 작은 곳에는 학습률을 높이는 방식을 이용합니다.

---
**Adam**은 RMSProp와 Momentum를 합쳐서 만든 최적화 알고리즘입니다. 기존의 SGD 방식의 문제를 해결하기 위해 나왔던 기법들의 장점을 모두 갖추었기 때문에 여러 최적화 알고리즘 중에서도 가장 강력한 성능을 보여주었습니다.

![enter link description here](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://t1.daumcdn.net/cfile/tistory/99DEF44B5AA814A30D)

실제로는 기존의 SGD 방식을 이용하더라도 주어진 데이터에  맞는 적절한 학습률 값을 찾은 경우에는 Adam을 이용한 것보다 더 좋은 결과를 내기도 하는데요. 하지만 사용자가 지정한 학습률이나 하이퍼 파라미터 값에 대해서 다른 Optimizer들보다 덜 민감하고 대체로 준수한 성능을 보여주기 때문에 안정성 측면에서 여전히 큰 인기를 끌고 있습니다.
